{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OlGm-uMyRtw"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    batch_size = 128\n",
    "    num_epochs = 300\n",
    "    workers = 4\n",
    "    seed = 2021\n",
    "    image_W = 64\n",
    "    image_H = 64\n",
    "    image_C = 3 ## number chanels  ngf = 64 #Size of feature maps in generator\n",
    "    download = True\n",
    "    dataroot = \"data\"\n",
    "    nz = 100 #latent random input vector\n",
    "    ngf = 64 #Size of feature maps in generator\n",
    "    ndf = 64 #Size of feature maps in discriminator\n",
    "    lr = 0.0002\n",
    "    device = 'cuda'  \n",
    "    sample_dir = \"./images/\"\n",
    "    \n",
    "if not os.path.exists(CFG.sample_dir):\n",
    "    os.makedirs(CFG.sample_dir)\n",
    "dataset = torchvision.datasets.CIFAR10(root=CFG.dataroot, download=CFG.download,\n",
    "                               transform=transforms.Compose([\n",
    "                                  transforms.Resize([CFG.image_H, CFG.image_W]),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDlQ5_NvzTPr"
   },
   "outputs": [],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5vacOJ4z9_P"
   },
   "outputs": [],
   "source": [
    "# denormalization image from range (-1)-1 to range 0-1 to display it\n",
    "def unnomalization(x):\n",
    "    batch_size_ = x.size(0)\n",
    "    x = x.view(batch_size_ , -1)\n",
    "    x -= x.min(1, keepdim=True)[0]\n",
    "    x /= x.max(1, keepdim=True)[0]\n",
    "    x = x.view(batch_size_, CFG.image_C, CFG.image_H, CFG.image_W)\n",
    "    return x\n",
    "\n",
    "img, label = dataset[0]\n",
    "img = unnomalization(img.unsqueeze(0)).squeeze(0) # The functions expects first dim to be batch_size\n",
    "print(\"Min: \", img.min().item())\n",
    "print(\"Max: \", img.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image sample with matplotlib\n",
    "def plot_torch_image(img, **kwargs):\n",
    "    '''\n",
    "    Input image is a torch tensor with the following dims (C,H,W)\n",
    "    To plot it with matplotlib, we need to change it to (H,W,C) \n",
    "    kwargs varaible is used to pass other parameters to 'imshow' function.\n",
    "    '''\n",
    "    plt.imshow(img.permute(1, 2, 0) , **kwargs)\n",
    "    \n",
    "plot_torch_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "data_loader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GAN \n",
    "состоит из двух глубоких сетей, генератора и дискриминатора. генератор создает изображения, прежде чем научиться его обучать. Поскольку дискриминатор представляет собой модель бинарной классификации, мы можем использовать функцию потери бинарной кросс-энтропии для количественной оценки того, насколько хорошо он может различать реальные и сгенерированные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HANijDxD20d1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nc, h, w, nz, ngf):\n",
    "        #ConvTranspose2d - BatchNorm - Relu -ConvTranspose2d - BatchNorm - Relu -ConvTranspose2d - BatchNorm - Relu \n",
    "        #ConvTranspose2d - BatchNorm - Relu - ConvTranspose2d - Tanh\n",
    "        \n",
    "        # ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding \n",
    "        super(Generator, self).__init__()\n",
    "        self.nc = nc\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.ngf = ngf\n",
    "        \n",
    "        self.linear = nn.Linear(nz, nc * h * w)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=nc, out_channels=ngf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.batch1 = nn.BatchNorm2d(ngf)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=ngf, out_channels=ngf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(ngf)\n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels=ngf, out_channels=ngf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.batch3 = nn.BatchNorm2d(ngf)\n",
    "        self.conv4 = nn.ConvTranspose2d(in_channels=ngf, out_channels=ngf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.batch4 = nn.BatchNorm2d(ngf) \n",
    "        self.conv5 = nn.ConvTranspose2d(in_channels=ngf, out_channels=nc, kernel_size=(3,3), stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs= x.shape[0]\n",
    "        x = self.linear(x)\n",
    "        x = x.view(bs, self.nc, self.h, self.w)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch2(x)\n",
    "        x = nn.ReLU()(x)     \n",
    "        x = self.conv3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = nn.ReLU()(x) \n",
    "        x = self.conv4(x)\n",
    "        x = self.batch4(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv5(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, h, w, ndf):\n",
    "    #conv2d - leaky - conv2d - batchnorm - leaky - conv2d - batchnorm - leaky - conv - batchnorm - leaky - conv2d\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=nc, out_channels=ndf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.l_relu1 = nn.LeakyReLU(0.1)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=ndf, out_channels=ndf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ndf)\n",
    "        self.l_relu2 = nn.LeakyReLU(0.1)\n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels=ndf, out_channels=ndf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf)\n",
    "        self.l_relu3 = nn.LeakyReLU(0.1)\n",
    "        self.conv4 = nn.ConvTranspose2d(in_channels=ndf, out_channels=ndf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(ndf)\n",
    "        self.l_relu4 = nn.LeakyReLU(0.1)\n",
    "        self.conv5 = nn.ConvTranspose2d(in_channels=ndf, out_channels=ndf, kernel_size=(3,3), stride=1, padding=1)\n",
    "        \n",
    "        self.linear = nn.Linear(262144, 1)\n",
    "    def forward(self, x):\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.l_relu1(x)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.l_relu2(x)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn2(x) \n",
    "        x = self.l_relu3(x)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn3(x) \n",
    "        x = self.l_relu4(x)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = self.conv5(x)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        x = self.linear(x)\n",
    "#         print(\"fwd: \", x.shape)\n",
    "        return nn.Sigmoid()(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MidVPKM-1_f"
   },
   "outputs": [],
   "source": [
    "# create new Generator model\n",
    "G = Generator(CFG.image_C, CFG.image_H, CFG.image_W, CFG.nz, CFG.ngf)\n",
    "# create new Discriminator model\n",
    "D = Discriminator(CFG.image_C, CFG.image_H, CFG.image_W, CFG.ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCKtI74k3-kY"
   },
   "outputs": [],
   "source": [
    "# show the output of model \n",
    "y = G(torch.randn(2, CFG.nz))\n",
    "with torch.no_grad():\n",
    "    for img in y:\n",
    "        img= img.reshape(CFG.image_C, CFG.image_H, CFG.image_W)\n",
    "        img = unnomalization(img.unsqueeze(0)).squeeze(0)\n",
    "        plt.figure()\n",
    "        plot_torch_image(img.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJWtCdgY5YZn"
   },
   "outputs": [],
   "source": [
    "# define the criterion is nn.BCELoss()\n",
    "loss_fn = nn.BCELoss()\n",
    "## Define the optimizer for generator and discrimator\n",
    "G_optim = torch.optim.Adam(G.parameters(), lr=CFG.lr)\n",
    "D_optim = torch.optim.Adam(D.parameters(), lr=CFG.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzFzJoO75jLc"
   },
   "outputs": [],
   "source": [
    "def reset_grad():\n",
    "    ## reset gradient for optimizer of generator and discrimator\n",
    "    G_optim.zero_grad()\n",
    "    D_optim.zero_grad()\n",
    "\n",
    "def train_discriminator(images):\n",
    "  \n",
    "    # Create the labels which are later used as input for the BCE loss\n",
    "    real_labels = torch.ones(CFG.batch_size, 1).to(CFG.device)\n",
    "    fake_labels = torch.zeros(CFG.batch_size, 1).to(CFG.device)\n",
    "    \n",
    "    outputs = D(images)\n",
    "    # Loss for real images\n",
    "    loss_r = loss_fn(outputs, real_labels)\n",
    "    real_score = outputs\n",
    "\n",
    "    # Loss for fake images\n",
    "    z = torch.randn(CFG.batch_size, CFG.nz).to(CFG.device)\n",
    "    fake_images = G(z)\n",
    "    outputs = D(fake_images)\n",
    "    loss_f = loss_fn(outputs, fake_labels)\n",
    "    fake_score = outputs\n",
    "    # Sum losses\n",
    "    d_loss = loss_r + loss_f\n",
    "    # Adjust the parameters using backprop\n",
    "    d_loss.backward()\n",
    "    # Compute gradients\n",
    "    D_optim.step()\n",
    "    # Reset gradients\n",
    "    reset_grad()\n",
    "\n",
    "    return d_loss, real_score, fake_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZ1u8nSA5qQK"
   },
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    # Generate fake images and calculate loss\n",
    "    # z = torch.randn(batch_size, latent_size).to(device)\n",
    "    z = torch.Tensor(np.random.normal(0, 1, (CFG.batch_size, CFG.nz))).to(CFG.device)\n",
    "    fake_images = G(z)\n",
    "    labels = torch.ones(CFG.batch_size, 1).to(CFG.device)\n",
    "    # calculate the generator loss\n",
    "    outputs = D(fake_images)\n",
    "    g_loss = loss_fn(outputs, labels)\n",
    "    # Backprop and optimize\n",
    "    g_loss.backward()\n",
    "    G_optim.step()\n",
    "    # Reset gradients\n",
    "    reset_grad()\n",
    "    return g_loss, fake_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x08qL8Bd5txk"
   },
   "outputs": [],
   "source": [
    "def save_fake_images(index):\n",
    "    sample_vectors = torch.randn(CFG.batch_size, CFG.nz , 1 , 1).to(CFG.device)\n",
    "    fake_images = G(sample_vectors)\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 3, 64, 64)\n",
    "    fake_fname = 'fake_images-{0:0=4d}.png'.format(index)\n",
    "    print('Saving', fake_fname)\n",
    "    save_image(unnomalization(fake_images), os.path.join(CFG.sample_dir, fake_fname), nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b52864sn5uUq"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "total_step = len(data_loader)\n",
    "d_losses, g_losses, real_scores, fake_scores = [], [], [], []\n",
    "G.to(CFG.device)\n",
    "D.to(CFG.device)\n",
    "for epoch in range(CFG.num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Load a batch & transform to vectors\n",
    "#         images = images.reshape(CFG.batch_size, -1).to(CFG.device)\n",
    "        images = images.to(CFG.device)\n",
    "        # Train the discriminator  \n",
    "        d_loss, real_score, fake_score = train_discriminator(images)\n",
    "        # Train the generator\n",
    "        g_loss, fake_images = train_generator()\n",
    "        # Inspect the losses\n",
    "        if (i+1) % 200 == 0:\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            real_scores.append(real_score.mean().item())\n",
    "            fake_scores.append(fake_score.mean().item())\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, CFG.num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    # Sample and save images\n",
    "    save_fake_images(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQKeNg5P7mY2"
   },
   "outputs": [],
   "source": [
    "Image(os.path.join(CFG.sample_dir, 'fake_images-0225.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EzucsLKBl1E"
   },
   "outputs": [],
   "source": [
    "plt.plot(d_losses, '-')\n",
    "plt.plot(g_losses, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DG9z_EZQDXr0"
   },
   "outputs": [],
   "source": [
    "plt.plot(real_scores, '-')\n",
    "plt.plot(fake_scores, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "plt.legend(['Real Score', 'Fake score'])\n",
    "plt.title('Scores');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yL2HkTUmDZWo"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./checkpoint/\"):\n",
    "    os.makedirs(\"./checkpoint/\")\n",
    "torch.save(\n",
    "    self.G.state_dict(),\n",
    "    \"checkpoint/generator_model.pth\")\n",
    "torch.save(\n",
    "    self.D.state_dict(),\n",
    "    \"checkpoint/discrimator_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02-DcGAN-cifar",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
